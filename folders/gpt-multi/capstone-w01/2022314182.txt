 AI has become not only a technology but also an essential part of our daily lives especially with the introduction of generative AI [1] . Today, I would like to talk about recent highly acclaimed on-device AI generation models.   The order of the presentation is as follows. First, Ill briefly explain what on-device video models are and what requirements are needed for them. Next, Ill describe which parts of the model were pruned to implement a lighter model and why those specific methods were used. Finally, Ill outline how the models performance was recovered through distillation, including a brief description of the methods used. For on device AI video generative model, fast inference and light weight model is of key importance. Recently, video generation models based on stable diffusion have shown impressive performance. These models are designed so that motion modules can be added to the stable diffusion framework in a plug-and-play manner. However, these models share the limitations of stable diffusion, such as slow inference time and large model sizes. To overcome these limitations, there have been significant efforts in the field of text-to-image models to develop faster and lighter models. For example, the LCM module [1]  was developed to enable faster inference and AnimateLCM [2] is a video generation model that supports such fast inference. However, there has been relatively little research on lightweight motion modules so far. In this study, we explore ways to build a more lightweight motion module in video generation models based on stable diffusion. Our research begins with the AnimateLCM model. More specifically, we try to reduce the redundancy by pruning unimportant blocks and compensate teacher model performance through distillation.