My research topic is on computer vision, which aims at making computer systems to see the world and reason about what they see. For instance, an autonomous driving system takes images from cameras and localizes cars on a road. Recently, research works on computer vision are heavily relying on deep learning algorithms. However, there is a major challenge in using deep learning in terms of efficiency. To train and utilize deep learning models, both heavy computational cost and large storage budget are often necessary for a good performance. I am currently working to develop more efficient computer vision models. There are many approaches to overcome the efficiency problem. Examples include synthesizing a small amount of samples to replace large-scale datasets, pruning unnecessary parameters in deep models, and quantizing the parameters to use lower bits . Dealing with the efficiency of deep models is becoming extremely important in order to utilize heavy models, such as large language models and deep generative models , in edge devices (e.g., mobile phones) where the resources are limited. I decided to delve into this field because I wanted to contribute to the fastly growing field of computer vision and I found the sequence of works that are introduced by researchers in this field fascinating. Training neural networks using large-scale datasets requires a large computational burden as well as significant storage capacity. In many cases these costs are only feasible for companies possessing large amounts of GPUs. To address the resources issue, a newly emerging field of dataset distillation synthesizes a small number of images, which are used to train neural networks while sacrificing minimal accuracy compared to those trained on real datasets with large quantities. The reduced dataset size saves storage capacity and the number of training iterations required to fully train networks on the dataset. Dataset distillation can facilitate various applications, including but not limited to neural architecture search, continual learning, privacy-preserving machine learning, and federated learning. For instance, neural architecture search aims to identify an optimal architecture that can provide the best performance on a task of interest. Searching for the optimal architecture requires training networks on a real dataset to estimate the performance of a specific architecture. In this context, a synthetic dataset can be used as a proxy to a real dataset, mitigating the cost of learning effectively.  Dataset distillation aims to synthesize a small number of images such that networks trained on them achieve comparable performance to those trained on real datasets, thereby reducing computational costs and storage budgets. The seminal work formulates dataset distillation as a bi-level optimization problem, where meta-gradients to update synthetic images are computed in outer loops, and networks are trained with the synthetic images in inner loops. One major concern is that the inner and outer loops require iteratively training networks and unrolling the training steps, respectively, at every update of synthetic images. To address heavy computational costs, one line of research substitutes iterative updates in the inner loops with a closed-form solution of kernel ridge regression using NTKs or convolutional features and, in turn, reformulates dataset distillation into a uni-level optimization problem. However, these approaches require processing all the synthetic images at every iteration, thereby consuming a substantial amount of VRAM memory, which hinders the use of kernel ridge regression for distilling high-resolution and/or large amounts of images. To mitigate the scalability issue, another line of research introduces surrogate objectives to train synthetic images. Dataset condensation (DC) minimizes the distance between one-step gradients of fixed network parameters derived from synthetic and real datasets. Matching training trajectories (MTT) extends DC and matches training trajectories obtained from synthetic and real images. Distribution matching (DM) instead minimizes the maximum mean discrepancy between synthetic and real images using neural features. Recently, Squeeze, recover and relabel (SRe2L) decouples the bi-level optimization problem for dataset distillation into two uni-level optimization problems by first training a network on a real dataset and subsequently synthesizing images such that the means and variances of input features to each layer in the trained network match the running means and variances, tracked during network training, of batch normalization.  The task of dataset distillation [ref], which aims to synthesize a few images as substitutes for numerous real images in training networks, in order to reduce computation and memory usage, has gained significant attention from researchers. Recently, Squeeze, Recover and Relabel (SRe2L) scaled up dataset distillation to 1000 different object categories and 224224 image resolution. Specifically, SRe2L includes network training on a real dataset and synthesizes images to match the batch normalization statistics of the trained network, followed by leveraging a knowledge distillation technique that stores soft labels for synthetic images. However, the justification for using the knowledge distillation method is insufficient. The technique includes augmenting images using various transformations and storing soft labels of all the augmented images, which costs a non-negligible storage budget. Also, SRe2L has limitations that have not been fully addressed with respect to dataset distillation. Specifically, this research project delves into the following questions:  Q1. To what extent does storing soft labels for images meet storage budget expectations for the task of dataset distillation? Q2. To what extent can the overfitting of synthetic images be prevented without causing severe performance degradations when training networks with various architectures? Q3. To what extent can images be synthesized in a manner that is both diverse and representative for training networks so that the images can provide rich yet important information?  By answering the aforementioned questions, this research project may ensure a fair comparison among different methods, and enhance the performance of dataset distillation in terms of network accuracy with various architectures trained on synthetic images. In turn, this work can facilitate future research on dataset distillation and provide a strong baseline for future research projects.  By synthesizing a few images and training networks on them, both the memory and computational costs resulting from operations on large-scale real images can be highly mitigated. To boost the performance of networks trained on synthetic images, a few works  store a soft label for each synthetic image, instead of storing one-hot labels for synthetic images. Recently, SRe2L  applies various data augmentation techniques for synthetic images and stores corresponding soft labels using the knowledge of a teacher network, which is trained on a large-scale real dataset. Although storing soft labels of diversely augmented synthetic images enhances the performance of networks trained on synthetic images, saving different data augmentation techniques and corresponding labels costs nontrivial memory usage. Moreover, whether the improvement of performance comes from synthesizing informative images or heavily using soft labels is unclear, hindering the analysis and comparison between different methods. To meet the storage budget expectations for dataset distillation and fairly compare different methods, the networks will be trained on at most one soft label only in this project. RQ2. In measuring the performance of networks trained on synthetic images, current practice is to train networks with different parameter initializations. However, even though synthetic images should ideally train various networks with different architectures, the network architecture for measuring the performance is fixed to one that is seen during image synthesis [ref]. While some works  measure the performance using a few network configurations, the comparison is restricted to one selected setting in terms of compression ratio and dataset. In this project, different methods will be compared with various network architectures for all the benchmarks. RQ3. While recent works  emphasizes the representativeness and diversity of synthetic images for dataset distillation, the works solely rely on the final accuracies of trained networks and do not directly analyze the representativeness and diversity of synthesized images. To this end, a measurement for quantifying the properties of different synthetic datasets can help analyze and improve methods for dataset distillation. Although defining such metrics is an open problem, the representativeness and diversity of synthetic images are measured by using confidence scores of pre-trained networks and gradient analysis in this project. RQ1. To what extent does storing soft labels for images meet storage budget expectations for the task of dataset distillation? RQ2. To what extent can the overfitting of synthetic images be prevented without causing severe performance degradations when training networks with various architectures? RQ3. To what extent can images be synthesized in a manner that is both diverse and representative, providing rich yet important information in training networks?  RQ1. By synthesizing a few images and training networks on them, both the memory and computational costs resulting from operations on large-scale real images can be highly mitigated. To boost the performance of networks trained on synthetic images, a few works  store a soft label for each synthetic image, instead of storing one-hot labels for synthetic images. Recently, SRe2L  applies various data augmentation techniques for synthetic images and stores corresponding soft labels using the knowledge of a teacher network, which is trained on a large-scale real dataset. Although storing soft labels of diversely augmented synthetic images enhances the performance of networks trained on synthetic images, saving different data augmentation techniques and corresponding labels costs nontrivial memory usage. Moreover, whether the improvement of performance comes from synthesizing informative images or heavily using soft labels is unclear, hindering the analysis and comparison between different methods. To meet the storage budget expectations for dataset distillation and fairly compare different methods, the networks will be trained on at most one soft label only in this project. RQ2. In measuring the performance of networks trained on synthetic images, current practice is to train networks with different parameter initializations. However, even though synthetic images should ideally train various networks with different architectures, the network architecture for measuring the performance is fixed to one that is seen during image synthesis [ref]. While some works  measure the performance using a few network configurations, the comparison is restricted to one selected setting in terms of compression ratio and dataset. In this project, different methods will be compared with various network architectures for all the benchmarks. RQ3. While recent works  emphasizes the representativeness and diversity of synthetic images for dataset distillation, the works solely rely on the final accuracies of trained networks and do not directly analyze the representativeness and diversity of synthesized images. To this end, a measurement for quantifying the properties of different synthetic datasets can help analyze and improve methods for dataset distillation. Although defining such metrics is an open problem, the representativeness and diversity of synthetic images are measured by using confidence scores of pre-trained networks and gradient analysis in this project.        Previous works suggest that storing soft labels improves the performance of trained networks at the cost of larger storage consumption. In response to RQ1, quantitative analysis of experimental settings with various compression ratios and image resolution will ideally reveal that the additional budget exceeds the range of feasible capacity (e.g., much larger than the disk usage to store synthetic images)  for dataset distillation. To address the substantial costs associated with soft labels, a new experimental protocol will be suggested using a single one-hot or soft label per image. Additionally, in answering RQ2, the protocol will consist of various network architectures to intensively test if synthetic images can train various network architectures effectively. Since the previous convention constitutes synthesizing images using networks via a single specified architecture, the synthesized images would not be effective for training networks with different architectures and the performance of those networks trained on synthetic images will, in expectation, degrade largely. Finally, to delve further into which synthetic images are effective for the new protocol in terms of RQ3, properties including diversity and representativeness will be tested with metrics used in literature for generative modeling. Previous works on dataset distillation suggested that diversity and representativeness are two important factors for dataset distillation, but the properties have been used for image synthesis only and no direct comparison among different methods was demonstrated. Both the diversity and representativeness of images synthesized using various methods are expected to correlate remarkably to the final performance of networks trained on the different sets of synthetic images. This project can further synthesize images that boost the metrics used to measure diversity and representativeness and consequently improve the performance in a newly proposed protocol. To test synthetic images on cross-architecture generalization, Generative Latent Distillation (GLaD) trains networks with several different architectures from a predefined pool using synthetic images, after which accuracies concerning validation splits are averaged and reported. However, the tests are still limited to predefined architectures from the model pool, which may not represent a specific requirement of the deployment from users. For instance, GLaD exploits the architectures of AlexNet, VGG-11, ResNet-18, and ViT. This pool does not include scenarios where one trains networks on mobile devices or trains modern networks with massive parameter counts. While this project will address different architectural configurations, measuring generalization across numerous models is impossible, and a suitable performance from the new model pool may need to be improved for certain scenarios. Besides, to analyze the diversity and representativeness of synthetic images, this research project will utilize the statistical properties of the synthetic images using pre-trained neural networks. However, as in the evaluation for generative models, assessing diversity and representativeness is an open problem that can be defined in numerous ways with different pros and cons. Nonetheless, quantitative analysis may facilitate future research to improve performance for dataset distillation.
