Vision is one of the most important senses for all animals including humans. Visual information helps animals to recognize things and to think. Computers and machines also need visual information to interact with objects near them and make correct decisions. However, unlike living things, the connection between seeing and thinking is not organized. In addition, vision systems in machines cannot recognize the world with desirable algorithms. I believe that bridging this gap between living things and machines is an important problem for pursuing the development of technology. That’s why I think that computer vision is an attractive research area and I’m studying computer vision. There are lots of problems to solve because research on computer vision with deep learning was conducted just a decade ago. I’m especially interested in multi-task learning which learns multiple objectives together to update one network. General deep learning networks perform just one task, but ideal networks can perform tasks. I believe that studies on multi-task learning can bridge the gap between living things and machines. Multi-task learning (MTL) for deep neural networks aims at training multiple tasks simultaneously into a network, while networks for single-task learning (STL) learn just one task at a time. MTL networks typically optimize loss functions as much as the number of tasks, causing an imbalance between the learning rate of task-specific decoders when the scales of losses are different. Loss balancing methods are studied to address the imbalance issue [ref]. They almost suggest weight designs multiplied by task-specific losses in the form of arithmetic mean. [ref] leverages learnable parameters,[ref] employs the learning speed of previous learning steps, and [ref] harnesses the rate between total loss and task-specific loss as the balancing weight. We show that the gradient of these methods can be represented as a unified form, and then strong loss balancing can be designed by extracting the benefits of other methods. Furthermore, we show that the loss balancing method can be expressed as a simple and well-known formula, Generalized mean. Loss balancing for multi-task learning (MTL) in deep learning aims to balance per-task losses, since scales of per-task losses are different, which causes a training imbalance where the task with a large-scale loss is dominantly learned among tasks. To address the issue, several works[1,3,4,5,6,7] propose weighting methods. Uncertainty weighting (UW) [1] designs weights dynamically with learnable parameters, adopting a concept of homoscedastic uncertainty [2]. Dynamic weighted averaging (DWA) [3] reflects a learning speed of per-task for multi-task training, which is formulated by the rate of change for a previous training loss with respect to a two-step previous training loss. Impartial multi-task learning IMTL [4] presents balancing impartially per-task loss with learnable parameters, supported by induced formulation from an assumption that the loss scales of each task are all equal to a constant value. In order to provide the same magnitude of per-task losses in multi-task learning (MTL), [5] presents a loss balancing method that establishes the same loss scales defined by a multiplication of a loss value and weight. Random weighting (RW) [6] proposes a loss balancing method wherein weights are randomly sampled from a distribution, such as Gaussian distribution, and is not much different from other loss balancing methods, showing statistical and theoretical reasons. A work of unit scalarization in [7] argues that equal weighting, the simplest design of loss weighting, is a suitable loss balancing method with proper regularization and stabilization such as early stopping though several elaborated-designed methods show their effectiveness.  [1] Kendall, Alex, Yarin Gal, and Roberto Cipolla. "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics." Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.  [2] Kendall, Alex, and Yarin Gal. "What uncertainties do we need in bayesian deep learning for computer vision?." Advances in neural information processing systems 30 (2017).   [3] Liu, Shikun, Edward Johns, and Andrew J. Davison. "End-to-end multi-task learning with attention." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.  [4] Liu, Liyang, et al. "Towards impartial multi-task learning." In International Conference on Learning Representations, 2021.  [5] Lee, Jae-Han, Chul Lee, and Chang-Su Kim. "Learning multiple pixelwise tasks based on loss scale balancing." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.  [6] Lin, Baijiong, et al. "Reasonable effectiveness of random weighting: A litmus test for multi-task learning." arXiv preprint arXiv:2111.10603 (2021).  [7] Kurin, Vitaly, et al. "In defense of the unitary scalarization for deep multi-task learning." Advances in Neural Information Processing Systems 35 (2022): 12169-12183. Multi-task learning (MTL) for deep neural networks aims to train several tasks simultaneously, which can lead to a negative interference among learning tasks at a shared-parameters due to different direction of updating for each task. To address the interference issue, numerous methods for multi-task optimization (MTO) have been considered to mitigate the conflicts of different updates [1, 2, 3, 4]. However, recently literatures [5,6] empirically observed that MTOs only marginally outperform or cannot perform significantly rather than linear scalarization, which is the simplest form of multi-task learning. The authors of [5,6] argue that no MTOs consistently yield performance improvements, though most MTOs have many times larger computational overheads and memory budgets than the linear scalarization. Furthermore, They also postulate that linear scalarization with proper regularizations such as early stopping or weight decay can overperform MTOs. Likewise, divided claims exist in the community, and the effectiveness of MTOs need to be validated. Therefore, in this paper, we investigate MTOs in terms of both theoretical and empirical analysis, and also present proper evaluation methods.   RQ1. Didn't MTOs really overperform equal weighting in terms of various evaluation metrics for multi-task learning? RQ2. Then, where does the disconnectivity come from between theoretical proofs and practical results of MTOs? RQ3. If the results across various settings such as dataset, learning rate, and  are inconsistent, what component  is really important for multi-task models and multi-task learning?  Multi-task learning (MTL) for deep neural networks aims to train several tasks simultaneously, which can cause negative interference among different tasks at shared parameters, since the gradients to update parameters are directed differently depending on the task. To address the interference issue, multi-task optimizations (MTO) [1, 2, 3, 4] [1-4] have been considered to mitigate the conflicts stemming from different update directions.  However, the recent literature [5,6] has empirically observed that although MTOs have many times larger computational overheads and memory budgets compared to linear scalarization, MTOs only marginally outperform or cannot perform significantly rather than significantly outperform approaches emphasizing linear scalarization, the simplest form of multi-task learning. In other words, no existing MTOs do not consistently yield performance improvement. To address the problem of the effectiveness of MTOs,  the authors in the works of [5,6] propose to explore the exploration of linear scalarizations with different weights, consisting of the Pareto front, which is the set of stationary points in multi-task learning (MTL).  They The researchers measure the average accuracy of multiple tasks over epochs on CelebA dataset [7], which is a homogenous multi-task problem. For heterogeneous multi-task problems such as the Cityscapes dataset [8], which is measured by via two independent evaluation metrics, relative performance improvement is leveraged following conventions [9, 10, 11].   Previous loss balancing methods for MTL [ref, ref, ref, ref] rely on the absolute scale of task-specific losses, although each loss associated with each task exhibits different lower bounds and scales. The proposed method, WGeM, can address the limitations via achievement-based weighting and adaptive aggregation. Following the convention [ref, ref], we conduct experiments on NYU-v2 [ref], Office-Home [ref], and QM9 [ref] datasets with Δp as evaluation metrics for MTL, which measures an overall improvements across different tasks as follows:  Formula for Δp        (1)  Since each dataset has different properties, a specific loss balancing method may encounter the difficulty of outperforming other methods on all these datasets. For example, the NYU-v2 dataset is a heterogeneous dataset, which has different kinds of tasks. However, Office-Home and QM9 datasets are homogeneous datasets, which have the same kinds of tasks and losses, although per-task losses have various scales due to their own difficulties. WGeM shows the highest Δp value over all datasets, which means that WGeM is an adaptive and scalable method to any multi-task datasets. Loss balancing for multi-task learning (MTL) has been addressed to control the weights of losses adaptively. However, previous loss balancing methods [ref, ref, ref] yield inconsistent performance on different datasets. For example, loss scale balancing [ref] achieves state-of-the-art on the NYU-v2 dataset [ref] while showing substantially deteriorated performance on the QM9 dataset [ref] and Office-Home dataset [ref]. The proposed method, weighted generalized mean (WGeM), achieves comparable performance on any dataset via an achievement-aware weight for each task and  a scale balancing term reflecting the current state of aggregated losses. However, WGeM requires additional knowledge such as a validation accuracy of each single task to measure the achievement at every epoch. Though this process is essential for measuring the performance of MTL, WGeM has computational overhead, which requires more hardware resources and training time. Furthermore, WGeM cannot ensure its theoretic validity in terms of a weight update of a task-shared encoder. Since loss balancing methods control the learning rates of each task-specific decoder, WGeM is able to control only a weight update of task-specific decoders. In addition, though the achievement-aware term and scale-balancing term explain complementary interaction between dependent and independent impact to loss scales, it is not sufficient to validate why WGeM outperforms other loss-balancing methods.
