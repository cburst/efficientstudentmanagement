  I am a freshman student of computer vision laboratory at Electrical and Electronic Engineering Collage. When I was intern at some institute, I recognized that computer vision contribute to solve and automate many problems. For example, We can detect traffic accident vehicles by increasing law-quality CCTV video’s resolution. I want to study and research in this field because I hope to work at institute for contributing to solve a crime problem in terms of computer vision. That’s why I chose this field to research.    I am interested in Neural Architecture Search(NAS), which is a technique for searching optimal neural architecture at each tasks. The reason of why people research about NAS is that it takes a lot of time to find optimal network, which is to reduce through automation like NAS. There are so many types of configuration variables for each network to consider(which is called ‘hyperparameters’), NAS helps to find the optimal values of variables automatically. I think NAS is important because finding the optimal network structure is increasing overall performance of each models of tasks.  -> after Grammarly    I am a freshman student in the computer vision laboratory at Electrical and Electronic Engineering College. While interning at some institutes, I recognized that computer vision contributes to solving and automating many problems. For example, We can detect traffic accident vehicles by increasing law-quality CCTV video resolution. I want to study and research in this field because I hope to work at an institute to contribute to solving a crime problem in terms of computer vision. That’s why I chose this field to research.    I am interested in Neural Architecture Search(NAS), which is a technique for searching optimal neural architecture at each task. People research NAS because it takes a lot of time to find an optimal network, which is to reduce through automation like NAS. There are so many types of configuration variables for each network to consider(which are called ‘hyperparameters’), that NAS helps to find the optimal values of variables automatically. I think NAS is important because finding the optimal network structure increases overall performance of each model of tasks.  I am interested in Neural Architecture Search (i.e., NAS; a technique aimed at finding the optimal architecture for specific tasks; Barret Zoph, 2017). Many IT companies and institutes of technology seek to make their products, such as online services, software, or automatic modules for hardware using machine learning by constructing neural architectures (Evelyn Herberg, 2023) appropriate for each product and task. However, neural architectures contain so many hyperparameters for learning, such as types of operations and the number of layers. In this situation, they must test all variations of the neural architecture to identify optimal configuration for the product or task, which can be time-consuming. Here, optimal means achieving high accuracy. However, when using the NAS technique, researchers can find the optimal neural architecture automatically. As a result, the NAS reduce the resource for searching for the optimal architecture dramatically. Because of that, developers can build their products faster and more efficiently.    I am interested in Neural Architecture Search (i.e., NAS; a technique aimed at finding the optimal architecture for specific tasks such as image classification or object detection)[1]. The NAS technique facilitates identifying automated solutions for neural architecture optimization. Unfortunately, substantial time and resource costs persist due to the training processes associated with existing NAS methods. Training parameters per architecture consumes considerable time, whereas NAS models necessitate training not only a single candidate architecture but also exploring all possible structure types. Therefore, recent research trends point towards ‘free-training’ NAS methods. For example, the University of Texas at Austin and Qualcomm have introduced ZiCo, a novel free-training proxy [2]. To support this concept theoretically, researchers first need to reveal specific gradient properties across different samples, the impact on convergence rates, and the generalization capacities in neural networks. To enhance existing free-training NAS methods, RMIT University researchers in Australia have proposed 'SWAP-NAS', which is the approach to free-training NAS that led to faster result generation using minimal resources [3]. However, the free-training NAS methods primarily focus on CNN or RNN architectures. The architectures cannot be applied to other architectures like Transformers [4]. Consequently, there is a burgeoning interest in NAS tailored specifically for Transformer architectures. Therefore, my future research endeavors will focus on developing free-training NAS tailored for Transformer architectures.          In recent years, researchers have explored training-free Neural Architecture Search (i.e., NAS; a technique aimed at finding optimal neural network architecture for specific tasks such as image classification or object detection) [1], methods for identifying an optimal neural network architecture in minimal time. To extend NAS optimization literature, this paper concerns a ‘SWAP(Sample-Wise Activation Patterns) NAS’ proxy, which delineates a novel and concise SotA(State-of-the-Art) method.[2] The key focus of the training-free NAS field lies in discovering scoring methods to analyze initial network parameters, such that the highest-scoring architecture is presumed to be optimal. In other words, The effectiveness of these scoring methods relies on their correlation with the architectures’ real performance metrics. Researchers behind ‘SWAP NAS’ also seek to propose a novel scoring method for training-free NAS, where the main research questions follow below.  RQ1. To what extent do the diversity of patterns in activation functions’ outputs correlate with ground-truth performance in neural architectures?  In pursuing this question, the researchers defined the SWAP score' as the number of patterns in activation functions’ outputs, presuming the architecture with the highest SWAP score to be optimal. As a result, they prove the correlation with the ReLU[3] activation function. However, given the variety of activation functions available (e.g., GELU, swish, etc. [4],[5]), not just ReLU, another question arises:  RQ2. To what extent do different types of activation functions correlate with the expressivity of the SWAP score?  As a result, I plan to investigate whether the SWAP score can constitute a sufficiently convincing indicator for other activation functions.               In order to address RQ1, researchers attempted to measure correlations between the pattern score of activation functions’ outputs and ground-truth performance using several rank correlation measurements such as Spearman’s rank correlation [1] and Kendall’s rank correlation [2]. These measurements facilitate correlation analysis, thereby constituting techniques for analyzing the linear relationship between two variables. The higher the correlation value is, the better the value expresses the performance of the network architecture proportionally. Therefore, researchers are trying to find measurements with a high correlation value.              In addition, measurements concerning the diversity of patterns in activation functions’ output are required. That’s why a lot of research is being done on the subject. For example, the most recent measurement, ' SWAP score’ [3], counted non-overlapping rows of a matrix with the column as each data sample and the row as activation output. As a result of an experiment [3], the higher an architecture’s SWAP score is, the better the architecture’s performance is. The SWAP score is applied only to the ReLU[4]. However, given the variety of activation functions available (e.g., GELU, swish [5],[6]), I plan to investigate whether the SWAP score can constitute a sufficiently convincing indicator for other activation functions, not just ReLU.                To address RQ1, researchers have attempted to measure correlations between the diversity of network weights and ground-truth performance using several rank correlation measurements, such as Spearman’s rank correlation [1] and Kendall’s rank correlation [2]. These measurements facilitate correlation analysis, thereby constituting techniques for analyzing linear relationship between two variables: the higher the correlation value, the better the value expresses the performance of the network architecture proportionally. Therefore, researchers continuously seek measurements with high correlation values.               Regarding RQ2, the search for valid measurements of network diversity is ongoing. Many researchers focus on the diversity of a network’s weights or gradients. Specifically, they anticipate that the higher the rank of the weight or gradient matrix increases, the higher the diversity of the network. The rank of a matrix equals the number of linearly independent rows (or columns) therein. However, calculating rank entails high computational costs. Instead, diversity is often assessed through an indicator called the 'nuclear norm,' which approximates this value based on mathematical theoretical grounds. For example, the DSS indicator [7] measures network diversity. Consequently, the method using the DSS indicator actually demonstrates a high correlation with ground-truth performance. Nonetheless, there may be a more logical way to measure network diversity, and discovering such a methodology is a focus of future work. If I find such a methodology, developers will be able to easily find an optimal network faster and with less computational cost, and as a result, the performance of services using that network will be much improved.     Recent research trends indicate a shift towards free training Neural Architecture Search (NAS) methods, which is a technique aims at finding optimal architectures for specific tasks such as image classification or object detection, because training parameters per architecture consume considerable time, while NAS models require training not only for a single candidate architecture but also for exploring all possible structure types. Free-training NAS predicts model performance by applying appropriately designed metrics using only the initial weights or gradients of the network, without further learning. However, free-training NAS methods primarily focus on CNN or RNN architectures. A limitation of those methods lies in their difficulty in application to others like the Transformer structures. The free-training NAS method may be applied to a transformer, but the reason it does not produce the expected results is due to the structural characteristics of Transformers. Unlike architectures such as CNN and RNN, Transformers can largely divide their internal structure into two modules: multi-head self-attention (MSA) and multi-layer perceptron (MLP). Since the structures of these two modules are very different, they must be calculated independently using different metrics. Subsequently, the independent metric results for each module must be approximately combined to ascertain an integrated network performance prediction score. Further research is expected to focus on designing metrics suitable for MSA and MLP, as well as presenting an appropriate methodology for their integration. This rationale underscores the selection of 'free-training TAS', which is specialized in Transformers, as the focus of my research topic.
