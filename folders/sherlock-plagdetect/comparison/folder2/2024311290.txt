Artificial intelligence[ref.] is an academic field, which aims to imitate human neuron systems. With a development of powerful GPUs and access to vast amounts of data, artificial intelligence has become a rapidly growing field. Recently, large language models such as ChatGPT[ref.] have been introduced using a vast amount of data and targets to generate the texts requested from the users. However, large language models such as ChatGPT[ref.] require tons of powerful GPUs and large memory for usage. Powerful GPUs and large memory usage make unsuitable for edge devices such as mobile phones. For this reason, researchers have begun exploring AI model compression for edge devices.  There are two main branches for model compression, Knowledge distillation[ref.] and Quantization[ref.]. The former introduces[ref.] a student model that imitates large language model with a lower model size and acts similarly as large model. However, the latter[ref.] makes the model’s weight to be an integer format for faster matrix multiplication and for lower computation costs. Ref. [] []  Diffusion models (Ho, 2020; Song, 2020) are generative models that use neural networks to predict the noise, which is added in the forward pass. For better understanding, the process of diffusion models can be described as follows: a model sequentially adds noise from the original image (i.e, forward pass) to create pure Gaussian noise and denoise the distribution of Gaussian noise sequentially to reconstruct the original image  (i.e, backward pass or reverse process). However, this process is repeated across numerous training iterations, leading to high computational costs. Consequently, diffusion models are unsuitable for edge devices (e.g, smartphones) due to hardware limitations (e.g, low computational capacity). To alleviate the hardware limitations of edge-devices, quantization technology (Li, 2021) (e.g, transforming floating points to integer points considering the distribution of weights or activations) have recently become the focus of researchers (Li, 2021). However, due to a unique property of diffusion models, which have numerous time steps, the activation distribution of diffusion models varies across all the time steps. Therefore, traditional quantizers(Li, 2021) that are designed only to consider a single time step, cannot be applied to diffusion models. For this reason, the goal of this research is to create a quantizer that can be applied to diffusion models, considering the activation distribution across all time steps.  Denoising diffusion models [1, 2], also dubbed as score-based generative models [3], have recently become a standard choice of image generation tasks, due to their high quality of generated samples with diversity. However, the denoising process is highly time-consuming as the denoising process repeats multiple time-steps equipped with heavy neural networks, which cause problems adapting to edge devices (e.g., smartphones) due to their low latency in real-world scenarios. To address this issue, model quantization (i.e., transforming full precision weights into integer weights) is a dominant technique for efficient computation. The first trial of quantizing diffusion models starts from [4], which simulates the quantizing process using full datasets. Nevertheless, it requires full datasets, which is not applicable in real scenarios. Therefore, [5] introduces an algorithm for establishing the calibration dataset, which is much smaller than full datasets. [6] extends [5] by introducing a novel quantizer that considering the characteristics of diffusion models that activation distribution varies across layers, while [7] extends [5] by combining the quantization error and the model’s noise. Both of them [6, 7] use static quantizers (i.e., same quantizer across all time steps) that do not consider the varying time-wise activation distribution. For this reason, dynamic quantization [8] can be an alternative for the varying time-wise activation distribution. Thus [9] adapted [8] using multi-layer perceptron, which predicts the quantization parameters for each independent time-steps. With the advancement of Deep Learning of neural networks [1], researchers have increasingly focused on expanding the size of models[2], driven by the potential of deep neural networks to accommodate vast amounts of information. Nevertheless, larger models present challenges in adaptation to edge devices, which typically possess limited resources. To adapt deep learning models to edge devices, two main approaches for model compression have emerged: scaling down the model size [3] and quantization [4] such that the preferred method for designing compressed models remains unverified. Therefore, this research project aims to evaluate the primary differences between scaling down the model size [3] and quantization [4], to consider their hardware compatibility and impact on model performance.  RQ1. To what extent do scaling down the model size [3] and quantization [4] influence hardware efficiency (e.g., computation efficiency)? RQ2. To what extent do scaling down the model size [3] and quantization [4] influence the model’s performance (e.g., accuracy)?  In this way, model engineers can select the suitable technology in relation to their domain characteristics given the trade-off between hardware efficiency and model performance. Based on the tables listed in [3] and [4], researchers may expect that for hardware efficiency, quantization is the best choice due to fast integer multiplication. In contrast, for the model performance, scaling down the model size [3] may be the preferred choice due to the abundant knowledge capacity preserved as 32-bit floating point weights which exhibit high precision.          In order to address the challenge of quantizing diffusion models, which is complicated by the varying activation distribution across multiple time steps and accumulating quantization errors across all time steps, two works [1],[2] were proposed to alleviate quantization errors. In [1], researchers asserted that the most significant quantization error impacts originate from the altered activation distributions caused by the model's skip-connections when quantizing diffusion models. Consequently, the tensors where the skip-connections of activation distributions occur were individually quantized. In [2], researchers discovered that the construction of an appropriate calibration dataset is crucial in determining model performance when quantizing diffusion models. Therefore, the Normally Distributed Time-Step Calibration (NDTC) algorithm was proposed for the development of the calibration dataset. Despite these works, quantifying diffusion models is still challenging. The difficulty comes from quantizing and measuring the performance of diffusion models using traditional generative model metrics, such as the FID (Frechet Inception Distance) [4] score and IS (Inception score) [5] score. These two metrics measure the difference between generated images and original images. However, quantization processes do not utilize original images but only the full-precision model in the training phase (i.e., training the quantization parameters). In other words, original data is not used in quantizing diffusion models. The unavailability of original data in quantizing the diffusion model limits the usage of traditional performance metrics. To address this issue, [3] proposed a performance metric comparing the differences between images generated by quantized models and those generated by the full-precision models called Frechet Inception Distance to 32-bit full-precision model (FID2FP32). To successfully quantize the diffusion models, even though various methods mentioned above have been proposed, there are still several ways to successfully quantize diffusion models. However, from the insights of the previous works, researchers can get inspiration for designing a successful quantization process for diffusion models.   Starting by considering RQ1, while previous research [1] and [2] have focused on the varying activation distribution across all time steps, visualization can also facilitate understanding such distribution. Upon visual examination, I noticed that the activation distribution of certain layers in the diffusion model exhibits symmetry with respect to zero (i.e., a bimodal distribution where the centered point is located on zero), which is sensitive to the quantization scenario. To illustrate, consider a standard quantizer applied to both a uniform distribution and a bimodal distribution. In the former case (i.e., applying quantizer into a uniform distribution), activation values will be quantized equally, resulting in reduces quantization error. However, in the latter case (i.e., applying quantizer into a bimodal distribution), the wide area between the two peaks of the distribution will lead to the quantizer being inefficiently utilized on less important parts, thereby reducing the precision of the values corresponding to the peaks. From the observation that certain layers' activation distribution appears as a  bimodal distribution and given the above reason why bimodal activation distributions are sensitive to quantization, I have tried to reshape the bimodal distribution into a specific distribution (e.g., uniform distribution, gaussian distribution), which is easier to quantize. However, given the intuition explained above (i.e., reshaping the bimodal distribution into a Gaussian distribution), quantization error still has not decreased due to unknown reasons. Therefore, an analysis is underway to identify these factors.  Beyond RQ1, to address RQ2, as discussed in the previous week, the inaccuracies of the FID metric [3], prompted the proposal of a new metric called FID2FP32 in [4] (i.e., measuring the difference between quantized model output and full-precision model output). Following experiments and result analysis, consistent outcomes were observed, wherein quantized models consistently underperformed compared to full precision models, where quantized models always underperformed compared to an ideal result of being full precision models. From this perspective, the FID2FP32 metric [4] increasingly represents a standard performance metric for evaluating quantized diffusion models.  The two main limitations of quantizing diffusion models can be described as follows:  1. varying activation distribution across all time steps. 2. accumulation of quantization error during the sampling process. Starting with the first problem, traditional quantization approaches are designed for the single-time step scenario (e.g., where the input domain doesn’t change). However, in the diffusion process, which involves sequentially injecting noise from the raw image (i.e., raw image to Gaussian noise), and reversely skimming off the noise (i.e., Gaussian noise to the raw image), a neural network (i.e., U-Net [1]) is used to skim off the noise at each time step (i.e., reverse process can be described as follows: pure Gaussian noise (XT ~N(0,I)) → XT-1 → … → raw image (X0 ), where a neural network (i.e., U-Net [1]) is used in each time step (i.e., symbol →) to skim off the noise and T indicates the total time steps.). In this process, the input domain of the U-Net [1] changes at every time step (i.e.,  XT , XT -1, ... ,X0), which affects variation in the activation distribution across all time steps (i.e., problem 1). This poses a challenge because traditional quantizers apply a single static quantizer per layer for any incoming input data, which is opposed to a diffusion scenario that has multiple time steps with varying input domains and limits the usage of applying a single static quantizer in the diffusion process. Even assuming that the first problem can be solved by a universal quantizer that can cover the varying activation distribution, there is still another limitation left, which is accumulating quantizer error (i.e., problem 2). Quantizing the neural network always involves quantization error due to transforming floating points into low-bit fixed integer points. In traditional quantization scenarios, which consider only a single time-step, quantization error may not be a big issue compared to a diffusion scenario. However, in the diffusion process, quantization error accumulates over multiple time steps, significantly magnifying its impact. For instance, if we assume quantization error equal to 1 for a single time step and a total of 1000 (T=1000) time steps. The total quantization error for the diffusion process becomes 1000 (1*1000). This accumulation of quantization error severely limits the applicability of traditional quantizers in diffusion scenarios. These limitations can be addressed by constructing a special quantizer, which can be fitted into the diffusion scenario that considers the numerous time steps.  
